{
	"name": "Ex 5 Task 2 - Model Training",
	"properties": {
		"folder": {
			"name": "Exercise 5"
		},
		"nbformat": 0,
		"nbformat_minor": 0,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Model training and registration\n",
					"This notebook show the process for training the model, converting the model to ONNX and uploading the ONNX model to Azure Storage."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Explore the training data\n",
					"The following cells load the source CSV file into a Spark DataFrame and create a temporary view that can be used to query the data with Spark SQL.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"df = spark.read.load('abfss://dev@<primary_storage>.dfs.core.windows.net/bronze/wwi-factsale.csv', format=\"csv\"\r\n",
					"## If header exists uncomment line bellow\r\n",
					", header=True, sep=\"|\"\r\n",
					")"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"df.createOrReplaceTempView(\"facts\")"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(spark.sql(\"SELECT * FROM facts WHERE `Customer Key` == '11' ORDER BY `Stock Item Key`\"))"
				],
				"execution_count": 37
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Predict Quantity given Customer Key and Stock Item Key\n",
					"In the following cells we load a subset of the data that just contains the fields needed for training. \n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import col\n",
					"df3 = spark.sql(\"SELECT double(`Customer Key`) as customerkey, double(`Stock Item Key`) as stockitemkey, double(`Quantity`) as quantity FROM facts\").where(col(\"quantity\").isNotNull())\n",
					"df3.cache()\n",
					"print(\"Number of records:\", df3.count())\n",
					"df3.show(10)"
				],
				"execution_count": 38
			},
			{
				"cell_type": "markdown",
				"source": [
					"Now, we split our DataFrame into training and testing DataFrames.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"trainingFraction = 0.7\n",
					"testingFraction = (1-trainingFraction)\n",
					"seed = 42\n",
					"\n",
					"# Split the dataframe into test and training dataframes\n",
					"df_train, df_test = df3.randomSplit([trainingFraction, testingFraction], seed=seed)"
				],
				"execution_count": 39
			},
			{
				"cell_type": "markdown",
				"source": [
					"Next, we package the data into the format expected by SPark ML's LinearRegression. It requires a DataFrame with two columns- `features` and a column with the labels to predict (`quantity` in this case).\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.ml.feature import VectorAssembler\n",
					"\n",
					"vectorAssembler = VectorAssembler(inputCols = ['customerkey', 'stockitemkey'], outputCol = 'features')"
				],
				"execution_count": 40
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"In the following cell, we create the LinearRegression model."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"from pyspark.ml.regression import LinearRegression\n",
					"\n",
					"lin_reg = LinearRegression(featuresCol = 'features', labelCol='quantity', maxIter = 10, regParam=0.3)"
				],
				"execution_count": 41
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Next, we build the training pipeline."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"from pyspark.ml import Pipeline\n",
					"\n",
					"stages = []\n",
					"stages += [vectorAssembler]\n",
					"stages += [lin_reg]\n",
					"\n",
					"partialPipeline = Pipeline().setStages(stages)"
				],
				"execution_count": 42
			},
			{
				"cell_type": "markdown",
				"source": [
					"In the following cell, we train our LinearRegression model.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"pipelineModel = partialPipeline.fit(df_train)\n",
					"\n",
					"print(\"Coefficients: \" + str(pipelineModel.stages[-1].coefficients))\n",
					"print(\"Intercept: \" + str(pipelineModel.stages[-1].intercept))"
				],
				"execution_count": 43
			},
			{
				"cell_type": "markdown",
				"source": [
					"With a trained model in hand, we can use it to make predictions against the test DataFrame.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"df_pred = pipelineModel.transform(df_test)\n",
					"\n",
					"from pyspark.sql.functions import rand\n",
					"display(df_pred.select([\"quantity\",  \"prediction\"]).orderBy(rand()).limit(5))"
				],
				"execution_count": 44
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Convert model to ONNX\n",
					"In the cells that follow, we convert the model to ONNX and show how an output of how ONNX represents the Spark ML model.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from onnxmltools import convert_sparkml\n",
					"from onnxmltools.convert.common.data_types import FloatTensorType\n",
					"\n",
					"initial_types = [ \n",
					"    (\"customerkey\", FloatTensorType([1, 1])),\n",
					"    (\"stockitemkey\", FloatTensorType([1, 1]))\n",
					"]"
				],
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"source": [
					"model_onnx = convert_sparkml(pipelineModel, 'sparkml GeneralizedLinearRegression', initial_types)\n",
					"model_onnx"
				],
				"execution_count": 54
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Upload the model to Azure Storage\n",
					"In the cells that follow we save the ONNX model to the storage of Spark driver node temporarily. Then we use the Azure Storage Python SDK to upload the ONNX model to Azure Storage.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"with open(\"model.onnx\", \"wb\") as f:\n",
					"    f.write(model_onnx.SerializeToString())"
				],
				"execution_count": 47
			},
			{
				"cell_type": "code",
				"source": [
					"connection_string = \"DefaultEndpointsProtocol=https;AccountName=<blob_storage>;AccountKey=<blob_storage_account_key>;EndpointSuffix=core.windows.net\"\n",
					"\n",
					"from azure.storage.blob import BlobClient\n",
					"\n",
					"blob = BlobClient.from_connection_string(conn_str=connection_string, container_name=\"models\", blob_name=\"onnx/model.onnx\")\n",
					"\n",
					"with open(\"./model.onnx\", \"rb\") as data:\n",
					"    blob.upload_blob(data,overwrite=True)"
				],
				"execution_count": 48
			}
		]
	}
}